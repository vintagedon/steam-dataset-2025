#!/usr/bin/env python3
# =================================================================================================
# File:          06-analyze-parquet-notebook03.py
# Project:       Steam Dataset 2025
# Repository:    https://github.com/vintagedon/steam-dataset-2025
# Author:        Don (vintagedon) | GitHub: https://github.com/vintagedon | ORCID: 0009-0008-7695-4093
# AI Collaborators: ChatGPT, Gemini
# License:       MIT
# Last Updated:  2025-10-06
#
# Executive Summary (non-developer audience)
#   Final QA and exploratory analysis for the model-ready Parquet output used by the
#   “Semantic Fingerprint” notebook. Confirms structure, label distributions, and
#   basic semantic behavior of the embedding features.
#
# Developer Notes (technical audience)
#   • Input: Parquet file generated by 05-export-parquet-notebook-03.py
#   • Validations: no NULLs; consistent embedding length; basic distribution checks
#   • Sanity checks: cosine similarity on normalized vectors for genre coherence
#   • Output: human-readable console tables via tabulate; non-zero exit on fatal conditions
#   • Scope: COMMENTING ONLY — no logic changes
# =================================================================================================

"""
Performs a final, comprehensive analysis of the model-ready Parquet file
for the "Semantic Fingerprint" notebook.

This script serves as a final quality assurance step and generates key
statistical insights to inform the notebook's implementation. It validates
the core data structure, analyzes the target variable distributions, and
probes the high-dimensional embedding data to confirm its integrity.
"""

# --- Standard library ---------------------------------------------------------------------------
import argparse
import sys
from pathlib import Path

# --- Third-party --------------------------------------------------------------------------------
# Non-dev: numpy/pandas for data handling; sklearn for normalization/similarity;
#          tabulate for readable console tables.
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
from tabulate import tabulate


def analyze_parquet(file_path: Path) -> None:
    """
    Load the Parquet file and run a sequence of structural checks and analyses.

    Non-dev:
      - Prints shape, cleanliness checks, label distributions, and a small sanity test on
        semantic similarity between a couple of Action games vs. one Casual game.

    Dev:
      - Asserts no NULLs and consistent vector dimensionality; exits non-zero on fatal issues.
      - Uses L2 normalization before cosine similarity to avoid magnitude bias post-PCA.
    """
    if not file_path.exists():
        print(f"❌ ERROR: File not found at '{file_path}'", file=sys.stderr)
        sys.exit(1)

    print(f"🔎 Analyzing file: {file_path}")
    df = pd.read_parquet(file_path)
    print(f"   File size: {file_path.stat().st_size / (1024*1024):.2f} MB")

    # --- 1. Core Data Validation -----------------------------------------------------------------
    print("\n" + "="*80)
    print("1. CORE DATA VALIDATION")
    print("="*80)
    
    print(f"\n▶️ Shape: {df.shape[0]:,} rows, {df.shape[1]} columns")

    # Dev: Fatal if any NULLs remain; exporter should have dropped/cleaned problematic rows.
    assert df.isnull().sum().sum() == 0, "TEST FAILED: Found unexpected NULL values."
    print("✅ PASSED: No NULL values found.")

    # Dev: All embedding vectors must share the same length for downstream stacking.
    emb_lens = df["emb"].map(len)
    assert emb_lens.nunique() == 1, "TEST FAILED: Inconsistent embedding vector lengths found."
    vec_len = emb_lens.iloc[0]
    print(f"✅ PASSED: All embedding vectors have a consistent length of {vec_len}.")

    # --- 2. Target Variable Analysis -------------------------------------------------------------
    print("\n" + "="*80)
    print("2. TARGET VARIABLE ANALYSIS")
    print("="*80)

    # Classification Target: primary_genre
    print("\n▶️ Classification Target: primary_genre")
    genre_counts = df["primary_genre"].value_counts()
    genre_dist = pd.DataFrame({
        'Genre': genre_counts.index,
        'Count': genre_counts.values,
        'Percentage': (genre_counts / len(df) * 100).values
    })
    print(tabulate(genre_dist, headers='keys', tablefmt='psql', showindex=False, floatfmt=".2f"))
    print("   - NOTE: Significant class imbalance exists. The model will require handling this (e.g., class_weight='balanced').")

    # Regression Target: ram_gb
    print("\n▶️ Regression Target: ram_gb (Minimum PC RAM in GB)")
    ram_stats = df["ram_gb"].describe(percentiles=[.25, .5, .75, .9, .95]).to_frame().T
    print(tabulate(ram_stats, headers='keys', tablefmt='psql', floatfmt=".2f"))
    print("   - NOTE: The distribution is right-skewed (mean > median), which is common for hardware specs.")
    print("   - A log transformation of this target may improve linear model performance.")

    # --- 3. Feature Analysis (Embeddings) --------------------------------------------------------
    print("\n" + "="*80)
    print("3. FEATURE ANALYSIS (EMBEDDINGS)")
    print("="*80)
    
    # Vector norms: verify magnitude properties; normalization is recommended before modeling.
    X_emb = np.vstack(df["emb"].to_numpy())
    norms = np.linalg.norm(X_emb, axis=1)
    print("\n▶️ Vector Norms (L2):")
    print(f"   - Mean: {norms.mean():.4f} | Std: {norms.std():.4f} | Min: {norms.min():.4f} | Max: {norms.max():.4f}")
    print("   - NOTE: Vectors are not unit-normalized. A StandardScaler in the model pipeline is recommended.")

    # Semantic Similarity Sanity Check (tiny sample)
    print("\n▶️ Semantic Similarity Sanity Check:")
    try:
        # Non-dev: Picks two Action games and one Casual game to compare.
        action_games = df[df['primary_genre'] == 'Action'].sample(2, random_state=42)
        casual_game = df[df['primary_genre'] == 'Casual'].sample(1, random_state=42)
        
        sample_df = pd.concat([action_games, casual_game])
        sample_embs = np.vstack(sample_df["emb"].to_numpy())
        
        # Dev: Normalize embeddings before cosine similarity (PCA may change magnitudes).
        sample_embs_normalized = normalize(sample_embs, norm='l2')
        sim_matrix = cosine_similarity(sample_embs_normalized)
        
        print("   - Comparing two 'Action' games against one 'Casual' game:")
        print(f"     - Game 1 (Action): appid {action_games.iloc[0]['appid']}")
        print(f"     - Game 2 (Action): appid {action_games.iloc[1]['appid']}")
        print(f"     - Game 3 (Casual): appid {casual_game.iloc[0]['appid']}")
        
        print("\n   - Cosine Similarity Matrix (on L2-normalized vectors):")
        sim_df = pd.DataFrame(
            sim_matrix, 
            index=['Action 1', 'Action 2', 'Casual 1'], 
            columns=['Action 1', 'Action 2', 'Casual 1']
        )
        print(tabulate(sim_df, headers='keys', tablefmt='psql', floatfmt=".4f"))
        print("   - As expected, the two Action games are semantically more similar to each other than to the Casual game.")
        print("✅ PASSED: Embeddings appear to capture meaningful semantic relationships.")

    except Exception as e:
        # Non-dev: If a genre is too small to sample, this check can be skipped gracefully.
        print(f"   - Could not perform similarity check (likely due to small sample size in a genre). Error: {e}")

    # --- 4. Cross-Target Analysis (Genre vs RAM) -------------------------------------------------
    print("\n" + "="*80)
    print("4. CROSS-TARGET ANALYSIS (Genre vs. RAM)")
    print("="*80)

    print("\n▶️ Average Minimum RAM (GB) per Genre:")
    genre_ram = df.groupby('primary_genre')['ram_gb'].mean().sort_values(ascending=False).to_frame()
    print(tabulate(genre_ram, headers=['Genre', 'Mean RAM (GB)'], tablefmt='psql', floatfmt=".2f"))
    print("\n   - INSIGHT: There is a clear and strong relationship between genre and hardware requirements.")
    print("   - 'Action', 'RPG', and 'Adventure' games demand significantly more RAM on average than 'Casual' games.")
    print("   - This confirms that a model should be able to find a signal in the data.")

    # --- 5. Conclusion & Recommendations ---------------------------------------------------------
    print("\n" + "="*80)
    print("5. CONCLUSION & RECOMMENDATIONS")
    print("="*80)
    print("✅ The dataset is clean, robust, and ready for modeling.")
    print("\nKey recommendations for the notebook implementation:")
    print("   1. **Classifier:** Use a method that handles class imbalance. `LogisticRegression(class_weight='balanced')` is an excellent start.")
    print("   2. **Regressor:** The `ram_gb` target is right-skewed. Consider modeling `log1p(ram_gb)` to stabilize variance for linear models.")
    print("   3. **Preprocessing:** The embeddings are not normalized. A `StandardScaler` should be the first step in both model pipelines.")
    print("   4. **Narrative:** The strong correlation between genre and RAM is a key finding. The notebook should build towards this, first showing that text can predict genre, then showing it can also predict a technical spec like RAM, likely because the language used to describe different genres is systematically different.")


def main() -> None:
    """
    CLI entrypoint.

    Usage (non-dev):
        python 06-analyze-parquet-notebook03.py /path/to/nb3_semantic_features.parquet

    Dev:
        - Argument parser prints defaults; type-safe Path conversion; exits non-zero on failure.
    """
    parser = argparse.ArgumentParser(
        description="Analyze the final Parquet file for the Semantic Fingerprint notebook.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        "file_path",
        help="Path to the nb3_semantic_features.parquet file."
    )
    args = parser.parse_args()
    
    file_path = Path(args.file_path)
    analyze_parquet(file_path)


if __name__ == "__main__":
    main()
