# =================================================================================================
# File:          analyze_notebook_2_data.py
# Project:       Steam Dataset 2025
# Author:        Generated by Gemini (Revised by User)
# Last Updated:  2025-10-05
#
# Purpose:
#   Performs a comprehensive analysis and validation of the pre-exported data files
#   for Notebook 2: "Semantic Game Discovery". This script serves as a final quality
#   assurance step and generates key insights to guide the notebook's narrative.
#
# Usage:
#   python analyze_notebook_2_data.py <path_to_data_directory>
#   Example: python analyze_notebook_2_data.py ./notebook_2_data_exports_20251005_140119/
#
# Revision Notes:
#   - Fixed KeyError by changing 'similarity_score' to 'cosine_similarity'.
#   - Added context for misleading statistics (free games, platform support).
#   - Removed flawed "Top 10 genres by review volume" analysis.
# =================================================================================================

import pandas as pd
import numpy as np
import json
import sys
from pathlib import Path

def analyze_data(data_dir: Path):
    """
    Main function to run all analysis and validation checks.
    """
    if not data_dir.exists():
        print(f"‚ùå ERROR: Data directory not found at '{data_dir}'")
        return

    print(f"üî¨ Starting analysis of data in: {data_dir}\n")

    # --- 1. Metadata Quality Check ---
    df_meta = pd.read_csv(data_dir / '01_game_embeddings_sample.csv')
    print("="*80)
    print("üìä 1. METADATA OVERVIEW (01_game_embeddings_sample.csv)")
    print("="*80)
    print(f"Total games: {len(df_meta):,}")
    print(f"\nColumn completeness (Null Values):")
    print(df_meta.isnull().sum())

    print(f"\nPrice Distribution:")
    print(df_meta['price_usd'].describe())
    # NOTE: The 'is_free' column is not in this sample. The count below is based on price being exactly 0,
    # which is not a reliable way to identify all free games as some have NULL prices.
    # The '04_pricing_strategy.csv' provides the accurate count from the full dataset.
    free_games_count = (df_meta['price_usd'] == 0).sum()
    print(f"\nFree games (in this sample where price is 0): {free_games_count:,} ({free_games_count / len(df_meta) * 100:.1f}%)")


    print(f"\nPlatform Support:")
    # INSIGHT: This sample is biased towards popular games, which have higher multi-platform support
    # than the dataset average. This is a key finding for the notebook narrative.
    platform_cols = ['mat_supports_windows', 'mat_supports_mac', 'mat_supports_linux']
    for col in platform_cols:
        support_count = df_meta[col].sum()
        print(f"  - {col}: {support_count:,} ({support_count / len(df_meta) * 100:.1f}%)")

    print(f"\nGenre Distribution (Top 15):")
    print(df_meta['primary_genre'].value_counts().head(15))

    print(f"\nReview Metrics:")
    print(f"  - Games with reviews: {df_meta['review_count'].notna().sum():,}")
    print(f"  - Average review count: {df_meta['review_count'].mean():.0f}")
    print(f"  - Average positive ratio: {df_meta['positive_ratio'].mean():.2%}")

    print(f"\nTemporal Coverage:")
    df_meta['release_year'] = pd.to_datetime(df_meta['release_date'], errors='coerce').dt.year
    print(f"  - Year range: {df_meta['release_year'].min():.0f} - {df_meta['release_year'].max():.0f}")
    print(f"\nGames by Decade:")
    print(pd.cut(df_meta['release_year'], bins=[1990, 2000, 2010, 2020, 2030], include_lowest=True).value_counts().sort_index())

    # --- 2. Genre Representatives Analysis ---
    df_genres = pd.read_csv(data_dir / '02_genre_representatives.csv')
    print("\n" + "="*80)
    print("üé≠ 2. GENRE REPRESENTATIVES (02_genre_representatives.csv)")
    print("="*80)
    print(f"Total entries: {len(df_genres):,}")
    print(f"Unique genres represented: {df_genres['genre'].nunique()}")

    print(f"\nGenres with < 100 games (underrepresented in sample):")
    genre_counts = df_genres['genre'].value_counts()
    underrepresented = genre_counts[genre_counts < 100]
    if underrepresented.empty:
        print("  - None. All genres have 100 representatives.")
    else:
        print(underrepresented)

    # --- 3. Semantic Search Quality Check ---
    with open(data_dir / '02_semantic_search_examples.json', 'r') as f:
        search_examples = json.load(f)
    print("\n" + "="*80)
    print("üîç 3. SEMANTIC SEARCH QUALITY (02_semantic_search_examples.json)")
    print("="*80)
    for example in search_examples:
        query = example['query']
        results = example['results']

        print(f"\nQuery: '{query}'")
        print(f"  Top 3 Matches:")
        for i, result in enumerate(results[:3], 1):
            # FIXED: Changed 'similarity_score' to 'cosine_similarity'
            sim_score = result['cosine_similarity']
            name = result['name']
            genres = result['genres'] or 'No Genre'
            print(f"    {i}. {name} ({genres}) - Similarity: {sim_score:.3f}")

        scores = [r['cosine_similarity'] for r in results]
        print(f"  Similarity Range for Top 10: {min(scores):.3f} - {max(scores):.3f}")

    # --- 4. Embedding Vector Validation ---
    embeddings = np.load(data_dir / '02_embeddings_vectors.npy')
    appids = pd.read_csv(data_dir / '02_embeddings_appids.csv')
    print("\n" + "="*80)
    print("üß† 4. EMBEDDING VECTORS (02_embeddings_vectors.npy)")
    print("="*80)
    print(f"Shape: {embeddings.shape}")
    print(f"Data type: {embeddings.dtype}")
    print(f"Memory size: {embeddings.nbytes / 1024 / 1024:.1f} MB")

    print(f"\nVector Statistics:")
    norms = np.linalg.norm(embeddings, axis=1)
    print(f"  - Mean L2 Norm: {norms.mean():.4f}")
    print(f"  - Std Dev of Norms: {norms.std():.4f}")
    print(f"  - All vectors normalized (within 0.001 tolerance): {np.allclose(norms, 1.0, atol=1e-3)}")

    print(f"\nSample vector (first 10 dimensions of first game):")
    print(embeddings[0, :10])

    print(f"\nAlignment Check:")
    match = len(embeddings) == len(appids) == len(df_meta)
    status = "‚úÖ" if match else "‚ùå"
    print(f"  - Embedding rows: {len(embeddings):,}")
    print(f"  - AppID mapping rows: {len(appids):,}")
    print(f"  - Metadata rows: {len(df_meta):,}")
    print(f"  - Alignment Match: {status}")
    if not match:
        print("  - ‚ùå CRITICAL: Misalignment detected between vectors and metadata!")

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python analyze_notebook_2_data.py <path_to_data_directory>")
        sys.exit(1)

    data_directory = Path(sys.argv[1])
    analyze_data(data_directory)
